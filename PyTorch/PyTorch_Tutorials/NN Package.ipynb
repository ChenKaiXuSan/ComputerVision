{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NN PACKAGE\n",
    "https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#nn-package"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Example 1:ConvNet\n",
    "All of your networks are derived from the base class nn.Module:\n",
    "\n",
    "- In the constructor, you declare all the layers you want to use.\n",
    "- In the forward function, you define how your model is going to be run, from input to output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1:ConvNet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MNISTConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MNISTConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50 ,10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.pool1(F.relu(self.conv1(input)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MNISTConvNet(\n  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=320, out_features=50, bias=True)\n  (fc2): Linear(in_features=50, out_features=10, bias=True)\n)\n"
     ]
    }
   ],
   "source": [
    "net = MNISTConvNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# Create a mini-batch containing a single sample of random data and send the sample through the ConvNet.\n",
    "input = torch.randn(1, 1, 28, 28)\n",
    "out = net(input)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.3561, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Define a dummy target label and compute error using a loss function.\n",
    "target = torch.tensor([3], dtype=torch.long)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "err = loss_fn(out, target)\n",
    "err.backward()\n",
    "\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "# Let’s access individual layer weights and gradients:\n",
    "print(net.conv1.weight.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.8325)\ntensor(0.1298)\n"
     ]
    }
   ],
   "source": [
    "print(net.conv1.weight.data.norm())\n",
    "print(net.conv1.weight.grad.data.norm())"
   ]
  },
  {
   "source": [
    "# Forward and Backward Function Hooks\n",
    "\n",
    "We’ve inspected the weights and the gradients. But how about inspecting / modifying the output and grad_output of a layer?\n",
    "\n",
    "We introduce hooks for this purpose.\n",
    "\n",
    "You can register a function on a Module or a Tensor. The hook can be a forward hook or a backward hook. The forward hook will be executed when a forward call is executed. The backward hook will be executed in the backward phase. Let’s look at an example."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inside Conv2d forward\n\ninput:  <class 'tuple'>\ninput[0]:  <class 'torch.Tensor'>\noutput:  <class 'torch.Tensor'>\n\ninput size: torch.Size([1, 10, 12, 12])\noutput size: torch.Size([1, 20, 8, 8])\noutput norm: tensor(12.0643)\n"
     ]
    }
   ],
   "source": [
    "# We register a forward hook on conv2 and print some information\n",
    "def printnorm(self, input, output):\n",
    "    # input is a tuple of packed inputs\n",
    "    # output is a Tensor. output.data is the Tensor we are interested\n",
    "    print('Inside ' + self.__class__.__name__ + ' forward')\n",
    "    print('')\n",
    "    print('input: ', type(input))\n",
    "    print('input[0]: ', type(input[0]))\n",
    "    print('output: ', type(output))\n",
    "    print('')\n",
    "    print('input size:', input[0].size())\n",
    "    print('output size:', output.data.size())\n",
    "    print('output norm:', output.data.norm())\n",
    "\n",
    "\n",
    "net.conv2.register_forward_hook(printnorm)\n",
    "\n",
    "out = net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Inside Conv2d forward\n\ninput:  <class 'tuple'>\ninput[0]:  <class 'torch.Tensor'>\noutput:  <class 'torch.Tensor'>\n\ninput size: torch.Size([1, 10, 12, 12])\noutput size: torch.Size([1, 20, 8, 8])\noutput norm: tensor(12.0643)\nInside Conv2d backward\nInside class:Conv2d\n\ngrad_input:  <class 'tuple'>\ngrad_input[0]:  <class 'torch.Tensor'>\ngrad_output:  <class 'tuple'>\ngrad_output[0]:  <class 'torch.Tensor'>\n\ngrad_input size: torch.Size([1, 10, 12, 12])\ngrad_output size: torch.Size([1, 20, 8, 8])\ngrad_input norm: tensor(0.0304)\n"
     ]
    }
   ],
   "source": [
    "# We register a backward hook on conv2 and print some information\n",
    "def printgradnorm(self, grad_input, grad_output):\n",
    "    print('Inside ' + self.__class__.__name__ + ' backward')\n",
    "    print('Inside class:' + self.__class__.__name__)\n",
    "    print('')\n",
    "    print('grad_input: ', type(grad_input))\n",
    "    print('grad_input[0]: ', type(grad_input[0]))\n",
    "    print('grad_output: ', type(grad_output))\n",
    "    print('grad_output[0]: ', type(grad_output[0]))\n",
    "    print('')\n",
    "    print('grad_input size:', grad_input[0].size())\n",
    "    print('grad_output size:', grad_output[0].size())\n",
    "    print('grad_input norm:', grad_input[0].norm())\n",
    "\n",
    "\n",
    "net.conv2.register_backward_hook(printgradnorm)\n",
    "\n",
    "out = net(input)\n",
    "err = loss_fn(out, target)\n",
    "err.backward()"
   ]
  },
  {
   "source": [
    "# Example 2:Recurrent Net\n",
    "Since the state of the network is held in the graph and not in the layers, you can simply create an nn.Linear and reuse it over and over again for the recurrence."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = data_size + hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "        hidden = self.i2h(input)\n",
    "        output = self.h2o(hidden)\n",
    "        return hidden, output\n",
    "\n",
    "rnn = RNN(50, 20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.3745, grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "batch_size = 10\n",
    "TIMESTEPS = 5\n",
    "\n",
    "# create some fake data\n",
    "batch = torch.randn(batch_size, 50)\n",
    "hidden = torch.zeros(batch_size, 20)\n",
    "target = torch.zeros(batch_size, 10)\n",
    "\n",
    "loss = 0\n",
    "for t in range(TIMESTEPS):\n",
    "    # yes! you can reuse the same network several times,\n",
    "    # sum up the losses, and call backward!\n",
    "    hidden, output = rnn(batch, hidden)\n",
    "    loss += loss_fn(output, target)\n",
    "\n",
    "loss.backward()\n",
    "loss"
   ]
  }
 ]
}