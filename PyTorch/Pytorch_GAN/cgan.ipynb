{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os \n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存生成的图片\n",
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=64, channels=1, img_size=32, latent_dim=100, lr=0.0002, n_classes=10, n_cpu=8, n_epochs=200, sample_interval=400)\n"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=10, help=\"number of classes for dataset\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=32, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=400, help=\"interval between image sampling\")\n",
    "opt = parser.parse_args(args=[])\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "print(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        def block(in_feat, out_feat, normailize = True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normailize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim + opt.n_classes, 128, normailize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))), # Return the product of array elements over a given axis.\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, nosie, labels):\n",
    "        # Concatenate label embedding and image to produce input\n",
    "        gen_input = torch.cat((self.label_emb(labels), nosie), -1)\n",
    "        img = self.model(gen_input)\n",
    "        img = img.view(img.size(0), *img_shape)\n",
    "        return img\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(opt.n_classes, opt.n_classes)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes + int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function \n",
    "adversarial_loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generator(\n  (label_emb): Embedding(10, 10)\n  (model): Sequential(\n    (0): Linear(in_features=110, out_features=128, bias=True)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Linear(in_features=128, out_features=256, bias=True)\n    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Linear(in_features=256, out_features=512, bias=True)\n    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Linear(in_features=512, out_features=1024, bias=True)\n    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n    (11): Linear(in_features=1024, out_features=1024, bias=True)\n    (12): Tanh()\n  )\n)\nDiscriminator(\n  (label_embedding): Embedding(10, 10)\n  (model): Sequential(\n    (0): Linear(in_features=1034, out_features=512, bias=True)\n    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): Dropout(p=0.4, inplace=False)\n    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n    (5): Linear(in_features=512, out_features=512, bias=True)\n    (6): Dropout(p=0.4, inplace=False)\n    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n    (8): Linear(in_features=512, out_features=1, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# initialize \n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "from torchsummary import summary\n",
    "# summary(generator, img_shape)\n",
    "# summary(discriminator, img_shape)\n",
    "print(generator)\n",
    "print(discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader \n",
    "# os.makedirs(\"/data/mnist\", exist_ok=True)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        \"../data/\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ),\n",
    "    ),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers \n",
    "optimizers_G = torch.optim.Adam(generator.parameters(), lr = opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizers_D = torch.optim.Adam(discriminator.parameters(), lr = opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(n_row, batches_done):\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, \"images/%d.png\" % batches_done, nrow=n_row, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ch 652/938] [D loss: 0.002893] [G loss: 0.034469]\n",
      "[Epoch 199/200] [Batch 653/938] [D loss: 0.003178] [G loss: 0.025879]\n",
      "[Epoch 199/200] [Batch 654/938] [D loss: 0.004586] [G loss: 0.043976]\n",
      "[Epoch 199/200] [Batch 655/938] [D loss: 0.008398] [G loss: 0.011165]\n",
      "[Epoch 199/200] [Batch 656/938] [D loss: 0.004145] [G loss: 0.022154]\n",
      "[Epoch 199/200] [Batch 657/938] [D loss: 0.008896] [G loss: 0.036886]\n",
      "[Epoch 199/200] [Batch 658/938] [D loss: 0.007467] [G loss: 0.010515]\n",
      "[Epoch 199/200] [Batch 659/938] [D loss: 0.006294] [G loss: 0.013596]\n",
      "[Epoch 199/200] [Batch 660/938] [D loss: 0.005489] [G loss: 0.029507]\n",
      "[Epoch 199/200] [Batch 661/938] [D loss: 0.005221] [G loss: 0.017718]\n",
      "[Epoch 199/200] [Batch 662/938] [D loss: 0.004728] [G loss: 0.024711]\n",
      "[Epoch 199/200] [Batch 663/938] [D loss: 0.004138] [G loss: 0.020711]\n",
      "[Epoch 199/200] [Batch 664/938] [D loss: 0.004185] [G loss: 0.025359]\n",
      "[Epoch 199/200] [Batch 665/938] [D loss: 0.004467] [G loss: 0.030983]\n",
      "[Epoch 199/200] [Batch 666/938] [D loss: 0.004052] [G loss: 0.022700]\n",
      "[Epoch 199/200] [Batch 667/938] [D loss: 0.004216] [G loss: 0.035796]\n",
      "[Epoch 199/200] [Batch 668/938] [D loss: 0.005454] [G loss: 0.015732]\n",
      "[Epoch 199/200] [Batch 669/938] [D loss: 0.005666] [G loss: 0.038360]\n",
      "[Epoch 199/200] [Batch 670/938] [D loss: 0.005164] [G loss: 0.019464]\n",
      "[Epoch 199/200] [Batch 671/938] [D loss: 0.004425] [G loss: 0.033142]\n",
      "[Epoch 199/200] [Batch 672/938] [D loss: 0.004226] [G loss: 0.020682]\n",
      "[Epoch 199/200] [Batch 673/938] [D loss: 0.004884] [G loss: 0.031252]\n",
      "[Epoch 199/200] [Batch 674/938] [D loss: 0.005767] [G loss: 0.015272]\n",
      "[Epoch 199/200] [Batch 675/938] [D loss: 0.004509] [G loss: 0.029057]\n",
      "[Epoch 199/200] [Batch 676/938] [D loss: 0.003669] [G loss: 0.024240]\n",
      "[Epoch 199/200] [Batch 677/938] [D loss: 0.004483] [G loss: 0.036114]\n",
      "[Epoch 199/200] [Batch 678/938] [D loss: 0.006321] [G loss: 0.014934]\n",
      "[Epoch 199/200] [Batch 679/938] [D loss: 0.003918] [G loss: 0.024500]\n",
      "[Epoch 199/200] [Batch 680/938] [D loss: 0.005509] [G loss: 0.038800]\n",
      "[Epoch 199/200] [Batch 681/938] [D loss: 0.005023] [G loss: 0.018231]\n",
      "[Epoch 199/200] [Batch 682/938] [D loss: 0.003163] [G loss: 0.029087]\n",
      "[Epoch 199/200] [Batch 683/938] [D loss: 0.004335] [G loss: 0.031039]\n",
      "[Epoch 199/200] [Batch 684/938] [D loss: 0.005200] [G loss: 0.014833]\n",
      "[Epoch 199/200] [Batch 685/938] [D loss: 0.004025] [G loss: 0.029050]\n",
      "[Epoch 199/200] [Batch 686/938] [D loss: 0.003956] [G loss: 0.028030]\n",
      "[Epoch 199/200] [Batch 687/938] [D loss: 0.003972] [G loss: 0.027958]\n",
      "[Epoch 199/200] [Batch 688/938] [D loss: 0.002865] [G loss: 0.037786]\n",
      "[Epoch 199/200] [Batch 689/938] [D loss: 0.003315] [G loss: 0.024772]\n",
      "[Epoch 199/200] [Batch 690/938] [D loss: 0.004570] [G loss: 0.045647]\n",
      "[Epoch 199/200] [Batch 691/938] [D loss: 0.004456] [G loss: 0.021950]\n",
      "[Epoch 199/200] [Batch 692/938] [D loss: 0.002549] [G loss: 0.030469]\n",
      "[Epoch 199/200] [Batch 693/938] [D loss: 0.003277] [G loss: 0.037756]\n",
      "[Epoch 199/200] [Batch 694/938] [D loss: 0.005195] [G loss: 0.017124]\n",
      "[Epoch 199/200] [Batch 695/938] [D loss: 0.003092] [G loss: 0.036548]\n",
      "[Epoch 199/200] [Batch 696/938] [D loss: 0.002788] [G loss: 0.027766]\n",
      "[Epoch 199/200] [Batch 697/938] [D loss: 0.003465] [G loss: 0.037362]\n",
      "[Epoch 199/200] [Batch 698/938] [D loss: 0.004463] [G loss: 0.018400]\n",
      "[Epoch 199/200] [Batch 699/938] [D loss: 0.004489] [G loss: 0.036549]\n",
      "[Epoch 199/200] [Batch 700/938] [D loss: 0.002614] [G loss: 0.027421]\n",
      "[Epoch 199/200] [Batch 701/938] [D loss: 0.002009] [G loss: 0.032036]\n",
      "[Epoch 199/200] [Batch 702/938] [D loss: 0.001849] [G loss: 0.034692]\n",
      "[Epoch 199/200] [Batch 703/938] [D loss: 0.002710] [G loss: 0.026552]\n",
      "[Epoch 199/200] [Batch 704/938] [D loss: 0.002094] [G loss: 0.036623]\n",
      "[Epoch 199/200] [Batch 705/938] [D loss: 0.003624] [G loss: 0.024948]\n",
      "[Epoch 199/200] [Batch 706/938] [D loss: 0.003233] [G loss: 0.040854]\n",
      "[Epoch 199/200] [Batch 707/938] [D loss: 0.006306] [G loss: 0.015248]\n",
      "[Epoch 199/200] [Batch 708/938] [D loss: 0.004068] [G loss: 0.038195]\n",
      "[Epoch 199/200] [Batch 709/938] [D loss: 0.004024] [G loss: 0.020128]\n",
      "[Epoch 199/200] [Batch 710/938] [D loss: 0.003422] [G loss: 0.028296]\n",
      "[Epoch 199/200] [Batch 711/938] [D loss: 0.003378] [G loss: 0.033986]\n",
      "[Epoch 199/200] [Batch 712/938] [D loss: 0.002418] [G loss: 0.033015]\n",
      "[Epoch 199/200] [Batch 713/938] [D loss: 0.002797] [G loss: 0.025980]\n",
      "[Epoch 199/200] [Batch 714/938] [D loss: 0.003062] [G loss: 0.033701]\n",
      "[Epoch 199/200] [Batch 715/938] [D loss: 0.002909] [G loss: 0.030441]\n",
      "[Epoch 199/200] [Batch 716/938] [D loss: 0.002349] [G loss: 0.032022]\n",
      "[Epoch 199/200] [Batch 717/938] [D loss: 0.002759] [G loss: 0.033432]\n",
      "[Epoch 199/200] [Batch 718/938] [D loss: 0.002283] [G loss: 0.031039]\n",
      "[Epoch 199/200] [Batch 719/938] [D loss: 0.003602] [G loss: 0.038190]\n",
      "[Epoch 199/200] [Batch 720/938] [D loss: 0.005790] [G loss: 0.015330]\n",
      "[Epoch 199/200] [Batch 721/938] [D loss: 0.006457] [G loss: 0.040076]\n",
      "[Epoch 199/200] [Batch 722/938] [D loss: 0.008802] [G loss: 0.009426]\n",
      "[Epoch 199/200] [Batch 723/938] [D loss: 0.005138] [G loss: 0.024764]\n",
      "[Epoch 199/200] [Batch 724/938] [D loss: 0.005313] [G loss: 0.022699]\n",
      "[Epoch 199/200] [Batch 725/938] [D loss: 0.004921] [G loss: 0.019739]\n",
      "[Epoch 199/200] [Batch 726/938] [D loss: 0.004098] [G loss: 0.023877]\n",
      "[Epoch 199/200] [Batch 727/938] [D loss: 0.004874] [G loss: 0.028123]\n",
      "[Epoch 199/200] [Batch 728/938] [D loss: 0.006612] [G loss: 0.014097]\n",
      "[Epoch 199/200] [Batch 729/938] [D loss: 0.004923] [G loss: 0.037022]\n",
      "[Epoch 199/200] [Batch 730/938] [D loss: 0.005515] [G loss: 0.018429]\n",
      "[Epoch 199/200] [Batch 731/938] [D loss: 0.004174] [G loss: 0.022238]\n",
      "[Epoch 199/200] [Batch 732/938] [D loss: 0.005125] [G loss: 0.037635]\n",
      "[Epoch 199/200] [Batch 733/938] [D loss: 0.005038] [G loss: 0.019406]\n",
      "[Epoch 199/200] [Batch 734/938] [D loss: 0.004594] [G loss: 0.030839]\n",
      "[Epoch 199/200] [Batch 735/938] [D loss: 0.002727] [G loss: 0.027191]\n",
      "[Epoch 199/200] [Batch 736/938] [D loss: 0.002826] [G loss: 0.034413]\n",
      "[Epoch 199/200] [Batch 737/938] [D loss: 0.003342] [G loss: 0.031711]\n",
      "[Epoch 199/200] [Batch 738/938] [D loss: 0.005864] [G loss: 0.014562]\n",
      "[Epoch 199/200] [Batch 739/938] [D loss: 0.005757] [G loss: 0.041602]\n",
      "[Epoch 199/200] [Batch 740/938] [D loss: 0.005725] [G loss: 0.014535]\n",
      "[Epoch 199/200] [Batch 741/938] [D loss: 0.002293] [G loss: 0.031183]\n",
      "[Epoch 199/200] [Batch 742/938] [D loss: 0.003163] [G loss: 0.032685]\n",
      "[Epoch 199/200] [Batch 743/938] [D loss: 0.002734] [G loss: 0.028137]\n",
      "[Epoch 199/200] [Batch 744/938] [D loss: 0.002184] [G loss: 0.032624]\n",
      "[Epoch 199/200] [Batch 745/938] [D loss: 0.003402] [G loss: 0.037058]\n",
      "[Epoch 199/200] [Batch 746/938] [D loss: 0.006442] [G loss: 0.017412]\n",
      "[Epoch 199/200] [Batch 747/938] [D loss: 0.004340] [G loss: 0.039832]\n",
      "[Epoch 199/200] [Batch 748/938] [D loss: 0.004393] [G loss: 0.019870]\n",
      "[Epoch 199/200] [Batch 749/938] [D loss: 0.003175] [G loss: 0.032488]\n",
      "[Epoch 199/200] [Batch 750/938] [D loss: 0.003385] [G loss: 0.031520]\n",
      "[Epoch 199/200] [Batch 751/938] [D loss: 0.003603] [G loss: 0.024894]\n",
      "[Epoch 199/200] [Batch 752/938] [D loss: 0.005082] [G loss: 0.039836]\n",
      "[Epoch 199/200] [Batch 753/938] [D loss: 0.007311] [G loss: 0.013512]\n",
      "[Epoch 199/200] [Batch 754/938] [D loss: 0.004377] [G loss: 0.036004]\n",
      "[Epoch 199/200] [Batch 755/938] [D loss: 0.005422] [G loss: 0.015151]\n",
      "[Epoch 199/200] [Batch 756/938] [D loss: 0.005238] [G loss: 0.023445]\n",
      "[Epoch 199/200] [Batch 757/938] [D loss: 0.005606] [G loss: 0.028493]\n",
      "[Epoch 199/200] [Batch 758/938] [D loss: 0.005950] [G loss: 0.014326]\n",
      "[Epoch 199/200] [Batch 759/938] [D loss: 0.003053] [G loss: 0.029763]\n",
      "[Epoch 199/200] [Batch 760/938] [D loss: 0.003044] [G loss: 0.037457]\n",
      "[Epoch 199/200] [Batch 761/938] [D loss: 0.002118] [G loss: 0.035436]\n",
      "[Epoch 199/200] [Batch 762/938] [D loss: 0.002452] [G loss: 0.032526]\n",
      "[Epoch 199/200] [Batch 763/938] [D loss: 0.002368] [G loss: 0.031190]\n",
      "[Epoch 199/200] [Batch 764/938] [D loss: 0.002788] [G loss: 0.031467]\n",
      "[Epoch 199/200] [Batch 765/938] [D loss: 0.002574] [G loss: 0.034173]\n",
      "[Epoch 199/200] [Batch 766/938] [D loss: 0.002083] [G loss: 0.030821]\n",
      "[Epoch 199/200] [Batch 767/938] [D loss: 0.002477] [G loss: 0.035816]\n",
      "[Epoch 199/200] [Batch 768/938] [D loss: 0.002496] [G loss: 0.027117]\n",
      "[Epoch 199/200] [Batch 769/938] [D loss: 0.003303] [G loss: 0.036058]\n",
      "[Epoch 199/200] [Batch 770/938] [D loss: 0.005655] [G loss: 0.017982]\n",
      "[Epoch 199/200] [Batch 771/938] [D loss: 0.006724] [G loss: 0.042945]\n",
      "[Epoch 199/200] [Batch 772/938] [D loss: 0.004864] [G loss: 0.017673]\n",
      "[Epoch 199/200] [Batch 773/938] [D loss: 0.003266] [G loss: 0.024248]\n",
      "[Epoch 199/200] [Batch 774/938] [D loss: 0.007482] [G loss: 0.036283]\n",
      "[Epoch 199/200] [Batch 775/938] [D loss: 0.008568] [G loss: 0.008833]\n",
      "[Epoch 199/200] [Batch 776/938] [D loss: 0.005111] [G loss: 0.017676]\n",
      "[Epoch 199/200] [Batch 777/938] [D loss: 0.007516] [G loss: 0.031874]\n",
      "[Epoch 199/200] [Batch 778/938] [D loss: 0.007745] [G loss: 0.009979]\n",
      "[Epoch 199/200] [Batch 779/938] [D loss: 0.006501] [G loss: 0.014991]\n",
      "[Epoch 199/200] [Batch 780/938] [D loss: 0.007882] [G loss: 0.029089]\n",
      "[Epoch 199/200] [Batch 781/938] [D loss: 0.005180] [G loss: 0.016458]\n",
      "[Epoch 199/200] [Batch 782/938] [D loss: 0.005547] [G loss: 0.018775]\n",
      "[Epoch 199/200] [Batch 783/938] [D loss: 0.004177] [G loss: 0.033622]\n",
      "[Epoch 199/200] [Batch 784/938] [D loss: 0.004184] [G loss: 0.025726]\n",
      "[Epoch 199/200] [Batch 785/938] [D loss: 0.003502] [G loss: 0.024707]\n",
      "[Epoch 199/200] [Batch 786/938] [D loss: 0.002730] [G loss: 0.029894]\n",
      "[Epoch 199/200] [Batch 787/938] [D loss: 0.002548] [G loss: 0.034441]\n",
      "[Epoch 199/200] [Batch 788/938] [D loss: 0.002640] [G loss: 0.029079]\n",
      "[Epoch 199/200] [Batch 789/938] [D loss: 0.003408] [G loss: 0.030668]\n",
      "[Epoch 199/200] [Batch 790/938] [D loss: 0.003071] [G loss: 0.031767]\n",
      "[Epoch 199/200] [Batch 791/938] [D loss: 0.002403] [G loss: 0.039050]\n",
      "[Epoch 199/200] [Batch 792/938] [D loss: 0.003067] [G loss: 0.025901]\n",
      "[Epoch 199/200] [Batch 793/938] [D loss: 0.002944] [G loss: 0.035484]\n",
      "[Epoch 199/200] [Batch 794/938] [D loss: 0.003184] [G loss: 0.024865]\n",
      "[Epoch 199/200] [Batch 795/938] [D loss: 0.004918] [G loss: 0.039140]\n",
      "[Epoch 199/200] [Batch 796/938] [D loss: 0.008912] [G loss: 0.010303]\n",
      "[Epoch 199/200] [Batch 797/938] [D loss: 0.003392] [G loss: 0.022387]\n",
      "[Epoch 199/200] [Batch 798/938] [D loss: 0.010058] [G loss: 0.035689]\n",
      "[Epoch 199/200] [Batch 799/938] [D loss: 0.006392] [G loss: 0.012070]\n",
      "[Epoch 199/200] [Batch 800/938] [D loss: 0.004651] [G loss: 0.018562]\n",
      "[Epoch 199/200] [Batch 801/938] [D loss: 0.006857] [G loss: 0.039237]\n",
      "[Epoch 199/200] [Batch 802/938] [D loss: 0.006353] [G loss: 0.012863]\n",
      "[Epoch 199/200] [Batch 803/938] [D loss: 0.005481] [G loss: 0.015315]\n",
      "[Epoch 199/200] [Batch 804/938] [D loss: 0.005120] [G loss: 0.029784]\n",
      "[Epoch 199/200] [Batch 805/938] [D loss: 0.003477] [G loss: 0.026888]\n",
      "[Epoch 199/200] [Batch 806/938] [D loss: 0.002990] [G loss: 0.029340]\n",
      "[Epoch 199/200] [Batch 807/938] [D loss: 0.003605] [G loss: 0.026611]\n",
      "[Epoch 199/200] [Batch 808/938] [D loss: 0.003314] [G loss: 0.029615]\n",
      "[Epoch 199/200] [Batch 809/938] [D loss: 0.002923] [G loss: 0.029086]\n",
      "[Epoch 199/200] [Batch 810/938] [D loss: 0.003496] [G loss: 0.034815]\n",
      "[Epoch 199/200] [Batch 811/938] [D loss: 0.004609] [G loss: 0.022602]\n",
      "[Epoch 199/200] [Batch 812/938] [D loss: 0.003981] [G loss: 0.036289]\n",
      "[Epoch 199/200] [Batch 813/938] [D loss: 0.002481] [G loss: 0.031631]\n",
      "[Epoch 199/200] [Batch 814/938] [D loss: 0.002815] [G loss: 0.028506]\n",
      "[Epoch 199/200] [Batch 815/938] [D loss: 0.003022] [G loss: 0.037614]\n",
      "[Epoch 199/200] [Batch 816/938] [D loss: 0.004379] [G loss: 0.022069]\n",
      "[Epoch 199/200] [Batch 817/938] [D loss: 0.003908] [G loss: 0.040120]\n",
      "[Epoch 199/200] [Batch 818/938] [D loss: 0.004925] [G loss: 0.017871]\n",
      "[Epoch 199/200] [Batch 819/938] [D loss: 0.002754] [G loss: 0.030908]\n",
      "[Epoch 199/200] [Batch 820/938] [D loss: 0.002779] [G loss: 0.035532]\n",
      "[Epoch 199/200] [Batch 821/938] [D loss: 0.002476] [G loss: 0.031121]\n",
      "[Epoch 199/200] [Batch 822/938] [D loss: 0.002596] [G loss: 0.029659]\n",
      "[Epoch 199/200] [Batch 823/938] [D loss: 0.003150] [G loss: 0.040420]\n",
      "[Epoch 199/200] [Batch 824/938] [D loss: 0.003588] [G loss: 0.021214]\n",
      "[Epoch 199/200] [Batch 825/938] [D loss: 0.003458] [G loss: 0.043552]\n",
      "[Epoch 199/200] [Batch 826/938] [D loss: 0.004266] [G loss: 0.021675]\n",
      "[Epoch 199/200] [Batch 827/938] [D loss: 0.002614] [G loss: 0.035614]\n",
      "[Epoch 199/200] [Batch 828/938] [D loss: 0.002936] [G loss: 0.031035]\n",
      "[Epoch 199/200] [Batch 829/938] [D loss: 0.003376] [G loss: 0.029371]\n",
      "[Epoch 199/200] [Batch 830/938] [D loss: 0.004811] [G loss: 0.036939]\n",
      "[Epoch 199/200] [Batch 831/938] [D loss: 0.003883] [G loss: 0.021805]\n",
      "[Epoch 199/200] [Batch 832/938] [D loss: 0.004138] [G loss: 0.035192]\n",
      "[Epoch 199/200] [Batch 833/938] [D loss: 0.003544] [G loss: 0.023098]\n",
      "[Epoch 199/200] [Batch 834/938] [D loss: 0.004940] [G loss: 0.036345]\n",
      "[Epoch 199/200] [Batch 835/938] [D loss: 0.008895] [G loss: 0.008585]\n",
      "[Epoch 199/200] [Batch 836/938] [D loss: 0.003230] [G loss: 0.027832]\n",
      "[Epoch 199/200] [Batch 837/938] [D loss: 0.004737] [G loss: 0.033687]\n",
      "[Epoch 199/200] [Batch 838/938] [D loss: 0.003831] [G loss: 0.020973]\n",
      "[Epoch 199/200] [Batch 839/938] [D loss: 0.004679] [G loss: 0.023177]\n",
      "[Epoch 199/200] [Batch 840/938] [D loss: 0.003238] [G loss: 0.028363]\n",
      "[Epoch 199/200] [Batch 841/938] [D loss: 0.003367] [G loss: 0.025995]\n",
      "[Epoch 199/200] [Batch 842/938] [D loss: 0.004109] [G loss: 0.035879]\n",
      "[Epoch 199/200] [Batch 843/938] [D loss: 0.005061] [G loss: 0.019546]\n",
      "[Epoch 199/200] [Batch 844/938] [D loss: 0.008225] [G loss: 0.041375]\n",
      "[Epoch 199/200] [Batch 845/938] [D loss: 0.008481] [G loss: 0.007893]\n",
      "[Epoch 199/200] [Batch 846/938] [D loss: 0.005741] [G loss: 0.021713]\n",
      "[Epoch 199/200] [Batch 847/938] [D loss: 0.007276] [G loss: 0.024153]\n",
      "[Epoch 199/200] [Batch 848/938] [D loss: 0.006983] [G loss: 0.013169]\n",
      "[Epoch 199/200] [Batch 849/938] [D loss: 0.005324] [G loss: 0.021708]\n",
      "[Epoch 199/200] [Batch 850/938] [D loss: 0.005810] [G loss: 0.027417]\n",
      "[Epoch 199/200] [Batch 851/938] [D loss: 0.005053] [G loss: 0.020064]\n",
      "[Epoch 199/200] [Batch 852/938] [D loss: 0.003080] [G loss: 0.028360]\n",
      "[Epoch 199/200] [Batch 853/938] [D loss: 0.003068] [G loss: 0.036679]\n",
      "[Epoch 199/200] [Batch 854/938] [D loss: 0.003331] [G loss: 0.021624]\n",
      "[Epoch 199/200] [Batch 855/938] [D loss: 0.003450] [G loss: 0.036171]\n",
      "[Epoch 199/200] [Batch 856/938] [D loss: 0.003337] [G loss: 0.027144]\n",
      "[Epoch 199/200] [Batch 857/938] [D loss: 0.003236] [G loss: 0.031390]\n",
      "[Epoch 199/200] [Batch 858/938] [D loss: 0.002521] [G loss: 0.031318]\n",
      "[Epoch 199/200] [Batch 859/938] [D loss: 0.002152] [G loss: 0.032000]\n",
      "[Epoch 199/200] [Batch 860/938] [D loss: 0.003521] [G loss: 0.026713]\n",
      "[Epoch 199/200] [Batch 861/938] [D loss: 0.004150] [G loss: 0.038864]\n",
      "[Epoch 199/200] [Batch 862/938] [D loss: 0.008438] [G loss: 0.010432]\n",
      "[Epoch 199/200] [Batch 863/938] [D loss: 0.003248] [G loss: 0.033279]\n",
      "[Epoch 199/200] [Batch 864/938] [D loss: 0.003645] [G loss: 0.028064]\n",
      "[Epoch 199/200] [Batch 865/938] [D loss: 0.002452] [G loss: 0.028996]\n",
      "[Epoch 199/200] [Batch 866/938] [D loss: 0.002762] [G loss: 0.032302]\n",
      "[Epoch 199/200] [Batch 867/938] [D loss: 0.003585] [G loss: 0.024833]\n",
      "[Epoch 199/200] [Batch 868/938] [D loss: 0.003831] [G loss: 0.039074]\n",
      "[Epoch 199/200] [Batch 869/938] [D loss: 0.005606] [G loss: 0.016188]\n",
      "[Epoch 199/200] [Batch 870/938] [D loss: 0.002544] [G loss: 0.034687]\n",
      "[Epoch 199/200] [Batch 871/938] [D loss: 0.003121] [G loss: 0.034006]\n",
      "[Epoch 199/200] [Batch 872/938] [D loss: 0.002909] [G loss: 0.023782]\n",
      "[Epoch 199/200] [Batch 873/938] [D loss: 0.003310] [G loss: 0.037673]\n",
      "[Epoch 199/200] [Batch 874/938] [D loss: 0.003981] [G loss: 0.020992]\n",
      "[Epoch 199/200] [Batch 875/938] [D loss: 0.003547] [G loss: 0.035598]\n",
      "[Epoch 199/200] [Batch 876/938] [D loss: 0.004023] [G loss: 0.020949]\n",
      "[Epoch 199/200] [Batch 877/938] [D loss: 0.004176] [G loss: 0.031427]\n",
      "[Epoch 199/200] [Batch 878/938] [D loss: 0.002903] [G loss: 0.027415]\n",
      "[Epoch 199/200] [Batch 879/938] [D loss: 0.002453] [G loss: 0.033877]\n",
      "[Epoch 199/200] [Batch 880/938] [D loss: 0.002856] [G loss: 0.030869]\n",
      "[Epoch 199/200] [Batch 881/938] [D loss: 0.003872] [G loss: 0.024410]\n",
      "[Epoch 199/200] [Batch 882/938] [D loss: 0.003883] [G loss: 0.038790]\n",
      "[Epoch 199/200] [Batch 883/938] [D loss: 0.004909] [G loss: 0.018300]\n",
      "[Epoch 199/200] [Batch 884/938] [D loss: 0.003454] [G loss: 0.026639]\n",
      "[Epoch 199/200] [Batch 885/938] [D loss: 0.004331] [G loss: 0.034430]\n",
      "[Epoch 199/200] [Batch 886/938] [D loss: 0.003043] [G loss: 0.024879]\n",
      "[Epoch 199/200] [Batch 887/938] [D loss: 0.003143] [G loss: 0.032305]\n",
      "[Epoch 199/200] [Batch 888/938] [D loss: 0.002350] [G loss: 0.030261]\n",
      "[Epoch 199/200] [Batch 889/938] [D loss: 0.003253] [G loss: 0.027829]\n",
      "[Epoch 199/200] [Batch 890/938] [D loss: 0.002806] [G loss: 0.036367]\n",
      "[Epoch 199/200] [Batch 891/938] [D loss: 0.003547] [G loss: 0.026057]\n",
      "[Epoch 199/200] [Batch 892/938] [D loss: 0.002500] [G loss: 0.036008]\n",
      "[Epoch 199/200] [Batch 893/938] [D loss: 0.002751] [G loss: 0.035733]\n",
      "[Epoch 199/200] [Batch 894/938] [D loss: 0.001831] [G loss: 0.034061]\n",
      "[Epoch 199/200] [Batch 895/938] [D loss: 0.003206] [G loss: 0.027645]\n",
      "[Epoch 199/200] [Batch 896/938] [D loss: 0.007144] [G loss: 0.045368]\n",
      "[Epoch 199/200] [Batch 897/938] [D loss: 0.010649] [G loss: 0.006113]\n",
      "[Epoch 199/200] [Batch 898/938] [D loss: 0.004375] [G loss: 0.023891]\n",
      "[Epoch 199/200] [Batch 899/938] [D loss: 0.005148] [G loss: 0.026806]\n",
      "[Epoch 199/200] [Batch 900/938] [D loss: 0.006290] [G loss: 0.012172]\n",
      "[Epoch 199/200] [Batch 901/938] [D loss: 0.005228] [G loss: 0.026959]\n",
      "[Epoch 199/200] [Batch 902/938] [D loss: 0.005342] [G loss: 0.018957]\n",
      "[Epoch 199/200] [Batch 903/938] [D loss: 0.005263] [G loss: 0.027605]\n",
      "[Epoch 199/200] [Batch 904/938] [D loss: 0.005387] [G loss: 0.016841]\n",
      "[Epoch 199/200] [Batch 905/938] [D loss: 0.006816] [G loss: 0.031142]\n",
      "[Epoch 199/200] [Batch 906/938] [D loss: 0.007267] [G loss: 0.010368]\n",
      "[Epoch 199/200] [Batch 907/938] [D loss: 0.004197] [G loss: 0.020568]\n",
      "[Epoch 199/200] [Batch 908/938] [D loss: 0.005200] [G loss: 0.043163]\n",
      "[Epoch 199/200] [Batch 909/938] [D loss: 0.005770] [G loss: 0.018431]\n",
      "[Epoch 199/200] [Batch 910/938] [D loss: 0.004619] [G loss: 0.019717]\n",
      "[Epoch 199/200] [Batch 911/938] [D loss: 0.005046] [G loss: 0.033891]\n",
      "[Epoch 199/200] [Batch 912/938] [D loss: 0.005854] [G loss: 0.015740]\n",
      "[Epoch 199/200] [Batch 913/938] [D loss: 0.005182] [G loss: 0.035088]\n",
      "[Epoch 199/200] [Batch 914/938] [D loss: 0.006039] [G loss: 0.016210]\n",
      "[Epoch 199/200] [Batch 915/938] [D loss: 0.003205] [G loss: 0.027193]\n",
      "[Epoch 199/200] [Batch 916/938] [D loss: 0.005063] [G loss: 0.034667]\n",
      "[Epoch 199/200] [Batch 917/938] [D loss: 0.005503] [G loss: 0.014642]\n",
      "[Epoch 199/200] [Batch 918/938] [D loss: 0.003638] [G loss: 0.028209]\n",
      "[Epoch 199/200] [Batch 919/938] [D loss: 0.003530] [G loss: 0.032778]\n",
      "[Epoch 199/200] [Batch 920/938] [D loss: 0.003117] [G loss: 0.023779]\n",
      "[Epoch 199/200] [Batch 921/938] [D loss: 0.002757] [G loss: 0.031890]\n",
      "[Epoch 199/200] [Batch 922/938] [D loss: 0.002713] [G loss: 0.031770]\n",
      "[Epoch 199/200] [Batch 923/938] [D loss: 0.003023] [G loss: 0.036807]\n",
      "[Epoch 199/200] [Batch 924/938] [D loss: 0.002453] [G loss: 0.032120]\n",
      "[Epoch 199/200] [Batch 925/938] [D loss: 0.002722] [G loss: 0.040770]\n",
      "[Epoch 199/200] [Batch 926/938] [D loss: 0.003867] [G loss: 0.025184]\n",
      "[Epoch 199/200] [Batch 927/938] [D loss: 0.003089] [G loss: 0.038390]\n",
      "[Epoch 199/200] [Batch 928/938] [D loss: 0.003524] [G loss: 0.023049]\n",
      "[Epoch 199/200] [Batch 929/938] [D loss: 0.003119] [G loss: 0.030543]\n",
      "[Epoch 199/200] [Batch 930/938] [D loss: 0.002889] [G loss: 0.032639]\n",
      "[Epoch 199/200] [Batch 931/938] [D loss: 0.003770] [G loss: 0.022411]\n",
      "[Epoch 199/200] [Batch 932/938] [D loss: 0.005669] [G loss: 0.039720]\n",
      "[Epoch 199/200] [Batch 933/938] [D loss: 0.005646] [G loss: 0.014554]\n",
      "[Epoch 199/200] [Batch 934/938] [D loss: 0.004777] [G loss: 0.037267]\n",
      "[Epoch 199/200] [Batch 935/938] [D loss: 0.005136] [G loss: 0.017927]\n",
      "[Epoch 199/200] [Batch 936/938] [D loss: 0.004095] [G loss: 0.035725]\n",
      "[Epoch 199/200] [Batch 937/938] [D loss: 0.004033] [G loss: 0.015976]\n"
     ]
    }
   ],
   "source": [
    "# training \n",
    "\n",
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad = False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.8), requires_grad = False)\n",
    "\n",
    "        # configure input \n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # train Generator\n",
    "\n",
    "        optimizers_G.zero_grad()\n",
    "\n",
    "        # sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        validity = discriminator(gen_imgs, gen_labels)\n",
    "        g_loss = adversarial_loss(validity, valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizers_G.step()\n",
    "\n",
    "        # train Discriminator\n",
    "\n",
    "        optimizers_D.zero_grad()\n",
    "\n",
    "        # loss for real images\n",
    "        validity_real = discriminator(real_imgs, labels)\n",
    "        d_real_loss = adversarial_loss(validity_real, valid)\n",
    "\n",
    "        # loss for fake images \n",
    "        validity_fake = discriminator(gen_imgs.detach(), gen_labels)\n",
    "        d_fake_loss = adversarial_loss(validity_fake, fake)\n",
    "\n",
    "        # total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizers_D.step()\n",
    "\n",
    "        print(\n",
    "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item())\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-15-095fb2b7395a>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-095fb2b7395a>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    if batches_done % opt.sample_interval == 0:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "batches_done = epoch * len(dataloader) + i\n",
    "if batches_done % opt.sample_interval == 0:\n",
    "    sample_image(n_row=10, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[1, 2, 4, 5],\n        [4, 3, 2, 9]])\nEmbedding(10, 3)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "input = torch.LongTensor([[1,2,4, 5], [4, 3, 2, 9]])\n",
    "print(input)\n",
    "embedding = nn.Embedding(10, 3)\n",
    "print(embedding)\n",
    "embedding(input).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([20, 100])\ntorch.Size([20, 100])\n"
     ]
    }
   ],
   "source": [
    "m = nn.BatchNorm1d(100)\n",
    "input = torch.randn(20, 100)\n",
    "print(input.shape)\n",
    "output = m(input)\n",
    "print(output.shape)"
   ]
  }
 ]
}