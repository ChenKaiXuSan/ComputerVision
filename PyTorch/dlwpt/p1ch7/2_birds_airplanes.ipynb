{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.7 64-bit",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "39bb97b9402fe54965182cdecdf1de398681b71a98db533a4cd01c0e11e7c257"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f431bf4e4f0>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "# Sets the seed for generating random numbers. Returns a torch.Generator object.\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train=True, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.4915, 0.4823, 0.4468),\n",
    "            (0.2470, 0.2435, 0.2616)\n",
    "        )\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.4915, 0.4823, 0.4468),\n",
    "            (0.2470, 0.2435, 0.2616)\n",
    "        )\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# 从cifar10里拿出飞机和鸟组成新的数据集，这个是datasets的子类\n",
    "label_map = {0:0, 2:1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "            for img, label in cifar10\n",
    "            if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "                for img, label in cifar10_val\n",
    "                if label in [0, 2]]\n",
    "\n",
    "len(cifar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "n_out = 2 \n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(\n",
    "        3072, # input features\n",
    "        512, # hidden layer size\n",
    "    ),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(\n",
    "        512, # hidden layer size \n",
    "        n_out,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # Returns a new tensor with the exponential of the elements of the input tensor input.\n",
    "    return torch.exp(x) / torch.exp(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "softmax(x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[0.0900, 0.2447, 0.6652],\n",
       "         [0.0900, 0.2447, 0.6652]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1) # 指定计算列数\n",
    "\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                [1.0, 2.0, 3.0]])\n",
    "\n",
    "softmax(x), x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.919844pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.919844\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2020-10-22T18:47:03.288428</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.1, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.919844 \nL 251.565 248.919844 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 225.041719 \nL 244.365 225.041719 \nL 244.365 7.601719 \nL 26.925 7.601719 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#p216374c668)\">\n    <image height=\"218\" id=\"image0be40674ef\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-7.041719\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"ma104b32708\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.3225\" xlink:href=\"#ma104b32708\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(27.14125 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"64.2975\" xlink:href=\"#ma104b32708\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g transform=\"translate(61.11625 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"98.2725\" xlink:href=\"#ma104b32708\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g transform=\"translate(91.91 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"132.2475\" xlink:href=\"#ma104b32708\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(125.885 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"166.2225\" xlink:href=\"#ma104b32708\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g transform=\"translate(159.86 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"200.1975\" xlink:href=\"#ma104b32708\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(193.835 239.640156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.1725\" xlink:href=\"#ma104b32708\" y=\"225.041719\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <g transform=\"translate(227.81 239.640156)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m740edf114b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m740edf114b\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m740edf114b\" y=\"44.974219\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 48.773437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m740edf114b\" y=\"78.949219\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 82.748437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m740edf114b\" y=\"112.924219\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 116.723437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m740edf114b\" y=\"146.899219\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m740edf114b\" y=\"180.874219\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 184.673437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m740edf114b\" y=\"214.849219\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 30 -->\n      <g transform=\"translate(7.2 218.648437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 225.041719 \nL 26.925 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 225.041719 \nL 244.365 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 225.041719 \nL 244.365 225.041719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.601719 \nL 244.365 7.601719 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p216374c668\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.601719\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch = img.view(-1).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4784, 0.5216]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# 未训练模型\n",
    "out = model(img_batch)\n",
    "# [airplanes, bird]的概率\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([1]), tensor([0.5216], grad_fn=<MaxBackward0>))"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "i, index = torch.max(out, dim=1)\n",
    "\n",
    "index, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "out = torch.tensor([\n",
    "    [0.6, 0.4],\n",
    "    [0.9, 0.1],\n",
    "    [0.3, 0.7],\n",
    "    [0.2, 0.8],\n",
    "])\n",
    "class_index = torch.tensor([0, 0, 1, 1]).unsqueeze(1)\n",
    "\n",
    "truth = torch.zeros((4,2))\n",
    "# Writes all values from the tensor src into self at the indices specified in the index tensor. \n",
    "truth.scatter_(dim=1, index=class_index, value=1.0)\n",
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.1500)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "def mse(out):\n",
    "    return ((out - truth) ** 2).sum(dim=1).mean()\n",
    "mse(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.6000],\n",
       "        [0.9000],\n",
       "        [0.7000],\n",
       "        [0.8000]])"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "out.gather(dim=1, index=class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.3024"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "0.6 * 0.7 * 0.9 * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.3024])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "def likelihood(out):\n",
    "    prod = 1.0\n",
    "    for x in out.gather(dim=1, index=class_index):\n",
    "        prod *= x\n",
    "    return prod \n",
    "\n",
    "likelihood(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1.1960])"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "# nll\n",
    "def neg_log_likelihood(out):\n",
    "    return -likelihood(out).log()\n",
    "\n",
    "neg_log_likelihood(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.0750, 0.1500, 0.2500, 0.4750])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "out0 = out.clone().detach() # Returns a new Tensor, detached from the current graph.\n",
    "out0[0] = torch.tensor([0.9, 0.1]) # more right\n",
    "\n",
    "out2 = out.clone().detach()\n",
    "out2[0] = torch.tensor([0.4, 0.6]) # slightly wrong\n",
    "\n",
    "out3 = out.clone().detach()\n",
    "out3[0] = torch.tensor([0.1, 0.9]) # very wrong\n",
    "\n",
    "mse_comparison = torch.tensor([mse(o) for o in [out0, out, out2, out3]])\n",
    "mse_comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-50.0000,   0.0000,  66.6667, 216.6667])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "((mse_comparison / mse_comparison[1]) - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.7905, 1.1960, 1.6015, 2.9878])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "nll_comparison = torch.tensor([neg_log_likelihood(o)\n",
    "                                for o in [out0, out, out2, out3]])\n",
    "nll_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-33.9016,   0.0000,  33.9016, 149.8121])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "((nll_comparison / nll_comparison[1]) - 1) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[  0., 104.]]), torch.Size([1, 2]), tensor([[0., 1.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "x = torch.tensor([[0.0, 104.0]])\n",
    "\n",
    "x, x.shape, softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-inf, 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "torch.log(softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-104.,    0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "torch.exp(log_softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-0.5216, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "img, label = cifar2[0]\n",
    "\n",
    "out = model(img.view(-1).unsqueeze(0))\n",
    "\n",
    "# 预测out和label的相似程度\n",
    "loss(out, torch.tensor([label]))"
   ]
  },
  {
   "source": [
    "# 在cpu上跑没有batch的数据太慢了，而且很蠢\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for img, label in cifar2:\n",
    "        out = model(img.view(-1).unsqueeze(0))\n",
    "        loss = loss_fn(out, torch.tensor([label]))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss %f\" % (epoch, float(loss)))"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss: 0.379931\n",
      "Epoch: 1, Loss: 0.510971\n",
      "Epoch: 2, Loss: 0.447792\n",
      "Epoch: 3, Loss: 0.653534\n",
      "Epoch: 4, Loss: 0.447541\n",
      "Epoch: 5, Loss: 0.291345\n",
      "Epoch: 6, Loss: 0.486947\n",
      "Epoch: 7, Loss: 0.270207\n",
      "Epoch: 8, Loss: 0.547912\n",
      "Epoch: 9, Loss: 0.238432\n",
      "Epoch: 10, Loss: 0.257918\n",
      "Epoch: 11, Loss: 0.356446\n",
      "Epoch: 12, Loss: 0.538808\n",
      "Epoch: 13, Loss: 0.226758\n",
      "Epoch: 14, Loss: 0.299214\n",
      "Epoch: 15, Loss: 0.311655\n",
      "Epoch: 16, Loss: 0.138688\n",
      "Epoch: 17, Loss: 0.289396\n",
      "Epoch: 18, Loss: 0.329703\n",
      "Epoch: 19, Loss: 0.491818\n",
      "Epoch: 20, Loss: 0.121620\n",
      "Epoch: 21, Loss: 0.239407\n",
      "Epoch: 22, Loss: 0.256800\n",
      "Epoch: 23, Loss: 0.362643\n",
      "Epoch: 24, Loss: 0.146391\n",
      "Epoch: 25, Loss: 0.224057\n",
      "Epoch: 26, Loss: 0.317490\n",
      "Epoch: 27, Loss: 0.242224\n",
      "Epoch: 28, Loss: 0.189765\n",
      "Epoch: 29, Loss: 0.247376\n",
      "Epoch: 30, Loss: 0.152678\n",
      "Epoch: 31, Loss: 0.370835\n",
      "Epoch: 32, Loss: 0.170147\n",
      "Epoch: 33, Loss: 0.153687\n",
      "Epoch: 34, Loss: 0.224271\n",
      "Epoch: 35, Loss: 0.091529\n",
      "Epoch: 36, Loss: 0.133091\n",
      "Epoch: 37, Loss: 0.123274\n",
      "Epoch: 38, Loss: 0.120532\n",
      "Epoch: 39, Loss: 0.040173\n",
      "Epoch: 40, Loss: 0.077782\n",
      "Epoch: 41, Loss: 0.119337\n",
      "Epoch: 42, Loss: 0.207575\n",
      "Epoch: 43, Loss: 0.036836\n",
      "Epoch: 44, Loss: 0.031540\n",
      "Epoch: 45, Loss: 0.073133\n",
      "Epoch: 46, Loss: 0.129386\n",
      "Epoch: 47, Loss: 0.092641\n",
      "Epoch: 48, Loss: 0.117379\n",
      "Epoch: 49, Loss: 0.246189\n",
      "Epoch: 50, Loss: 0.077856\n",
      "Epoch: 51, Loss: 0.225766\n",
      "Epoch: 52, Loss: 0.092543\n",
      "Epoch: 53, Loss: 0.174685\n",
      "Epoch: 54, Loss: 0.207112\n",
      "Epoch: 55, Loss: 0.109654\n",
      "Epoch: 56, Loss: 0.086468\n",
      "Epoch: 57, Loss: 0.230137\n",
      "Epoch: 58, Loss: 0.095006\n",
      "Epoch: 59, Loss: 0.045218\n",
      "Epoch: 60, Loss: 0.051149\n",
      "Epoch: 61, Loss: 0.053290\n",
      "Epoch: 62, Loss: 0.069704\n",
      "Epoch: 63, Loss: 0.060160\n",
      "Epoch: 64, Loss: 0.063453\n",
      "Epoch: 65, Loss: 0.040084\n",
      "Epoch: 66, Loss: 0.057160\n",
      "Epoch: 67, Loss: 0.048737\n",
      "Epoch: 68, Loss: 0.143186\n",
      "Epoch: 69, Loss: 0.022943\n",
      "Epoch: 70, Loss: 0.072831\n",
      "Epoch: 71, Loss: 0.047167\n",
      "Epoch: 72, Loss: 0.079771\n",
      "Epoch: 73, Loss: 0.026133\n",
      "Epoch: 74, Loss: 0.023044\n",
      "Epoch: 75, Loss: 0.009115\n",
      "Epoch: 76, Loss: 0.034259\n",
      "Epoch: 77, Loss: 0.026321\n",
      "Epoch: 78, Loss: 0.020228\n",
      "Epoch: 79, Loss: 0.041864\n",
      "Epoch: 80, Loss: 0.015119\n",
      "Epoch: 81, Loss: 0.065924\n",
      "Epoch: 82, Loss: 0.028201\n",
      "Epoch: 83, Loss: 0.028321\n",
      "Epoch: 84, Loss: 0.016032\n",
      "Epoch: 85, Loss: 0.052133\n",
      "Epoch: 86, Loss: 0.019488\n",
      "Epoch: 87, Loss: 0.017666\n",
      "Epoch: 88, Loss: 0.030908\n",
      "Epoch: 89, Loss: 0.041762\n",
      "Epoch: 90, Loss: 0.015059\n",
      "Epoch: 91, Loss: 0.013321\n",
      "Epoch: 92, Loss: 0.015525\n",
      "Epoch: 93, Loss: 0.031511\n",
      "Epoch: 94, Loss: 0.020010\n",
      "Epoch: 95, Loss: 0.018195\n",
      "Epoch: 96, Loss: 0.014210\n",
      "Epoch: 97, Loss: 0.022998\n",
      "Epoch: 98, Loss: 0.006648\n",
      "Epoch: 99, Loss: 0.011917\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss 0.326693\n",
      "Epoch: 1, Loss 0.394001\n",
      "Epoch: 2, Loss 0.328502\n",
      "Epoch: 3, Loss 0.178092\n",
      "Epoch: 4, Loss 0.364719\n",
      "Epoch: 5, Loss 0.454674\n",
      "Epoch: 6, Loss 0.548658\n",
      "Epoch: 7, Loss 0.352326\n",
      "Epoch: 8, Loss 0.636690\n",
      "Epoch: 9, Loss 0.390800\n",
      "Epoch: 10, Loss 0.355435\n",
      "Epoch: 11, Loss 0.222640\n",
      "Epoch: 12, Loss 0.202111\n",
      "Epoch: 13, Loss 0.172411\n",
      "Epoch: 14, Loss 0.271181\n",
      "Epoch: 15, Loss 0.219212\n",
      "Epoch: 16, Loss 0.613340\n",
      "Epoch: 17, Loss 0.204373\n",
      "Epoch: 18, Loss 0.314177\n",
      "Epoch: 19, Loss 0.236539\n",
      "Epoch: 20, Loss 0.180212\n",
      "Epoch: 21, Loss 0.297169\n",
      "Epoch: 22, Loss 0.147050\n",
      "Epoch: 23, Loss 0.395495\n",
      "Epoch: 24, Loss 0.217830\n",
      "Epoch: 25, Loss 0.346799\n",
      "Epoch: 26, Loss 0.122175\n",
      "Epoch: 27, Loss 0.310462\n",
      "Epoch: 28, Loss 0.117975\n",
      "Epoch: 29, Loss 0.108739\n",
      "Epoch: 30, Loss 0.289741\n",
      "Epoch: 31, Loss 0.162111\n",
      "Epoch: 32, Loss 0.343498\n",
      "Epoch: 33, Loss 0.067240\n",
      "Epoch: 34, Loss 0.145010\n",
      "Epoch: 35, Loss 0.326151\n",
      "Epoch: 36, Loss 0.075940\n",
      "Epoch: 37, Loss 0.142854\n",
      "Epoch: 38, Loss 0.160553\n",
      "Epoch: 39, Loss 0.324590\n",
      "Epoch: 40, Loss 0.072031\n",
      "Epoch: 41, Loss 0.051867\n",
      "Epoch: 42, Loss 0.088255\n",
      "Epoch: 43, Loss 0.124502\n",
      "Epoch: 44, Loss 0.222815\n",
      "Epoch: 45, Loss 0.123690\n",
      "Epoch: 46, Loss 0.098991\n",
      "Epoch: 47, Loss 0.060653\n",
      "Epoch: 48, Loss 0.073836\n",
      "Epoch: 49, Loss 0.079879\n",
      "Epoch: 50, Loss 0.142623\n",
      "Epoch: 51, Loss 0.051674\n",
      "Epoch: 52, Loss 0.034129\n",
      "Epoch: 53, Loss 0.056633\n",
      "Epoch: 54, Loss 0.028228\n",
      "Epoch: 55, Loss 0.052061\n",
      "Epoch: 56, Loss 0.038059\n",
      "Epoch: 57, Loss 0.031738\n",
      "Epoch: 58, Loss 0.147242\n",
      "Epoch: 59, Loss 0.061352\n",
      "Epoch: 60, Loss 0.063811\n",
      "Epoch: 61, Loss 0.032167\n",
      "Epoch: 62, Loss 0.023267\n",
      "Epoch: 63, Loss 0.036451\n",
      "Epoch: 64, Loss 0.104467\n",
      "Epoch: 65, Loss 0.020089\n",
      "Epoch: 66, Loss 0.053532\n",
      "Epoch: 67, Loss 0.045569\n",
      "Epoch: 68, Loss 0.010450\n",
      "Epoch: 69, Loss 0.064581\n",
      "Epoch: 70, Loss 0.017714\n",
      "Epoch: 71, Loss 0.031624\n",
      "Epoch: 72, Loss 0.034551\n",
      "Epoch: 73, Loss 0.039539\n",
      "Epoch: 74, Loss 0.021144\n",
      "Epoch: 75, Loss 0.008193\n",
      "Epoch: 76, Loss 0.055955\n",
      "Epoch: 77, Loss 0.024295\n",
      "Epoch: 78, Loss 0.013203\n",
      "Epoch: 79, Loss 0.014967\n",
      "Epoch: 80, Loss 0.009689\n",
      "Epoch: 81, Loss 0.029018\n",
      "Epoch: 82, Loss 0.010666\n",
      "Epoch: 83, Loss 0.010389\n",
      "Epoch: 84, Loss 0.100264\n",
      "Epoch: 85, Loss 0.026471\n",
      "Epoch: 86, Loss 0.042246\n",
      "Epoch: 87, Loss 0.019857\n",
      "Epoch: 88, Loss 0.075709\n",
      "Epoch: 89, Loss 0.101994\n",
      "Epoch: 90, Loss 0.018711\n",
      "Epoch: 91, Loss 0.050570\n",
      "Epoch: 92, Loss 0.009092\n",
      "Epoch: 93, Loss 0.012307\n",
      "Epoch: 94, Loss 0.006614\n",
      "Epoch: 95, Loss 0.024892\n",
      "Epoch: 96, Loss 0.011157\n",
      "Epoch: 97, Loss 0.006983\n",
      "Epoch: 98, Loss 0.024198\n",
      "Epoch: 99, Loss 0.019835\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 512), # 更改中间层特征数量\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64 64\n",
      "128 128\n",
      "192 192\n",
      "256 256\n",
      "320 320\n",
      "384 384\n",
      "448 448\n",
      "512 512\n",
      "576 576\n",
      "640 640\n",
      "704 704\n",
      "768 768\n",
      "832 832\n",
      "896 896\n",
      "960 960\n",
      "1024 1024\n",
      "1088 1088\n",
      "1152 1152\n",
      "1216 1216\n",
      "1280 1280\n",
      "1344 1344\n",
      "1408 1408\n",
      "1472 1472\n",
      "1536 1536\n",
      "1600 1600\n",
      "1664 1664\n",
      "1728 1728\n",
      "1792 1792\n",
      "1856 1856\n",
      "1920 1920\n",
      "1984 1984\n",
      "2048 2048\n",
      "2112 2112\n",
      "2176 2176\n",
      "2240 2240\n",
      "2304 2304\n",
      "2368 2368\n",
      "2432 2432\n",
      "2496 2496\n",
      "2560 2560\n",
      "2624 2624\n",
      "2688 2688\n",
      "2752 2752\n",
      "2816 2816\n",
      "2880 2880\n",
      "2944 2944\n",
      "3008 3008\n",
      "3072 3072\n",
      "3136 3136\n",
      "3200 3200\n",
      "3264 3264\n",
      "3328 3328\n",
      "3392 3392\n",
      "3456 3456\n",
      "3520 3520\n",
      "3584 3584\n",
      "3648 3648\n",
      "3712 3712\n",
      "3776 3776\n",
      "3840 3840\n",
      "3904 3904\n",
      "3968 3968\n",
      "4032 4032\n",
      "4096 4096\n",
      "4160 4160\n",
      "4224 4224\n",
      "4288 4288\n",
      "4352 4352\n",
      "4416 4416\n",
      "4480 4480\n",
      "4544 4544\n",
      "4608 4608\n",
      "4672 4672\n",
      "4736 4736\n",
      "4800 4800\n",
      "4864 4864\n",
      "4928 4928\n",
      "4992 4992\n",
      "5056 5056\n",
      "5120 5120\n",
      "5184 5184\n",
      "5248 5248\n",
      "5312 5312\n",
      "5375 5376\n",
      "5439 5440\n",
      "5503 5504\n",
      "5567 5568\n",
      "5631 5632\n",
      "5695 5696\n",
      "5759 5760\n",
      "5823 5824\n",
      "5887 5888\n",
      "5951 5952\n",
      "6015 6016\n",
      "6079 6080\n",
      "6143 6144\n",
      "6207 6208\n",
      "6271 6272\n",
      "6335 6336\n",
      "6399 6400\n",
      "6463 6464\n",
      "6527 6528\n",
      "6591 6592\n",
      "6655 6656\n",
      "6719 6720\n",
      "6782 6784\n",
      "6846 6848\n",
      "6910 6912\n",
      "6974 6976\n",
      "7038 7040\n",
      "7102 7104\n",
      "7166 7168\n",
      "7230 7232\n",
      "7294 7296\n",
      "7358 7360\n",
      "7422 7424\n",
      "7486 7488\n",
      "7550 7552\n",
      "7614 7616\n",
      "7678 7680\n",
      "7742 7744\n",
      "7806 7808\n",
      "7870 7872\n",
      "7934 7936\n",
      "7998 8000\n",
      "8062 8064\n",
      "8126 8128\n",
      "8190 8192\n",
      "8254 8256\n",
      "8318 8320\n",
      "8381 8384\n",
      "8445 8448\n",
      "8509 8512\n",
      "8573 8576\n",
      "8637 8640\n",
      "8701 8704\n",
      "8765 8768\n",
      "8829 8832\n",
      "8893 8896\n",
      "8957 8960\n",
      "9021 9024\n",
      "9085 9088\n",
      "9149 9152\n",
      "9213 9216\n",
      "9277 9280\n",
      "9341 9344\n",
      "9405 9408\n",
      "9469 9472\n",
      "9533 9536\n",
      "9597 9600\n",
      "9661 9664\n",
      "9725 9728\n",
      "9789 9792\n",
      "9853 9856\n",
      "9917 9920\n",
      "9981 9984\n",
      "9997 10000\n",
      "accuracy: 0.999700\n"
     ]
    }
   ],
   "source": [
    "# 验证训练集准确率 不洗牌\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        print(correct, total)\n",
    "\n",
    "print(\"accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "46 64\n99 128\n148 192\n203 256\n251 320\n302 384\n349 448\n401 512\n456 576\n507 640\n559 704\n608 768\n665 832\n716 896\n770 960\n821 1024\n870 1088\n915 1152\n972 1216\n1025 1280\n1080 1344\n1133 1408\n1184 1472\n1237 1536\n1289 1600\n1345 1664\n1399 1728\n1452 1792\n1508 1856\n1566 1920\n1623 1984\n1636 2000\naccuracy: 0.818000\n"
     ]
    }
   ],
   "source": [
    "# 验证集准确率\n",
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        print(correct, total)\n",
    "\n",
    "print(\"accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crossentropyloss combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2)\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss: 0.467440\n",
      "Epoch: 1, Loss: 0.546665\n",
      "Epoch: 2, Loss: 0.235942\n",
      "Epoch: 3, Loss: 0.552646\n",
      "Epoch: 4, Loss: 0.438913\n",
      "Epoch: 5, Loss: 0.405718\n",
      "Epoch: 6, Loss: 0.651299\n",
      "Epoch: 7, Loss: 0.365505\n",
      "Epoch: 8, Loss: 0.202632\n",
      "Epoch: 9, Loss: 0.583006\n",
      "Epoch: 10, Loss: 0.558033\n",
      "Epoch: 11, Loss: 0.271199\n",
      "Epoch: 12, Loss: 0.385183\n",
      "Epoch: 13, Loss: 0.509575\n",
      "Epoch: 14, Loss: 0.445199\n",
      "Epoch: 15, Loss: 0.341095\n",
      "Epoch: 16, Loss: 0.433102\n",
      "Epoch: 17, Loss: 0.606591\n",
      "Epoch: 18, Loss: 0.261556\n",
      "Epoch: 19, Loss: 0.388963\n",
      "Epoch: 20, Loss: 0.314825\n",
      "Epoch: 21, Loss: 0.200921\n",
      "Epoch: 22, Loss: 0.257796\n",
      "Epoch: 23, Loss: 0.440425\n",
      "Epoch: 24, Loss: 0.600442\n",
      "Epoch: 25, Loss: 0.460757\n",
      "Epoch: 26, Loss: 0.788530\n",
      "Epoch: 27, Loss: 0.327609\n",
      "Epoch: 28, Loss: 0.194952\n",
      "Epoch: 29, Loss: 0.240534\n",
      "Epoch: 30, Loss: 0.114583\n",
      "Epoch: 31, Loss: 1.155974\n",
      "Epoch: 32, Loss: 0.187521\n",
      "Epoch: 33, Loss: 0.234862\n",
      "Epoch: 34, Loss: 0.171355\n",
      "Epoch: 35, Loss: 0.366816\n",
      "Epoch: 36, Loss: 0.610443\n",
      "Epoch: 37, Loss: 0.072456\n",
      "Epoch: 38, Loss: 0.066273\n",
      "Epoch: 39, Loss: 0.171720\n",
      "Epoch: 40, Loss: 0.129383\n",
      "Epoch: 41, Loss: 0.061699\n",
      "Epoch: 42, Loss: 0.116120\n",
      "Epoch: 43, Loss: 0.075789\n",
      "Epoch: 44, Loss: 0.049374\n",
      "Epoch: 45, Loss: 0.258949\n",
      "Epoch: 46, Loss: 0.246652\n",
      "Epoch: 47, Loss: 0.129849\n",
      "Epoch: 48, Loss: 0.012059\n",
      "Epoch: 49, Loss: 0.019218\n",
      "Epoch: 50, Loss: 0.060403\n",
      "Epoch: 51, Loss: 0.090743\n",
      "Epoch: 52, Loss: 0.006393\n",
      "Epoch: 53, Loss: 0.111232\n",
      "Epoch: 54, Loss: 0.179592\n",
      "Epoch: 55, Loss: 0.043882\n",
      "Epoch: 56, Loss: 0.017224\n",
      "Epoch: 57, Loss: 0.017628\n",
      "Epoch: 58, Loss: 0.039903\n",
      "Epoch: 59, Loss: 0.057875\n",
      "Epoch: 60, Loss: 0.055044\n",
      "Epoch: 61, Loss: 0.135146\n",
      "Epoch: 62, Loss: 0.082499\n",
      "Epoch: 63, Loss: 0.006621\n",
      "Epoch: 64, Loss: 0.004000\n",
      "Epoch: 65, Loss: 0.012674\n",
      "Epoch: 66, Loss: 0.028389\n",
      "Epoch: 67, Loss: 0.006284\n",
      "Epoch: 68, Loss: 0.117043\n",
      "Epoch: 69, Loss: 0.017507\n",
      "Epoch: 70, Loss: 0.003696\n",
      "Epoch: 71, Loss: 0.016874\n",
      "Epoch: 72, Loss: 0.005582\n",
      "Epoch: 73, Loss: 0.015321\n",
      "Epoch: 74, Loss: 0.053633\n",
      "Epoch: 75, Loss: 0.019364\n",
      "Epoch: 76, Loss: 0.021044\n",
      "Epoch: 77, Loss: 0.007350\n",
      "Epoch: 78, Loss: 0.001609\n",
      "Epoch: 79, Loss: 0.009264\n",
      "Epoch: 80, Loss: 0.009703\n",
      "Epoch: 81, Loss: 0.009048\n",
      "Epoch: 82, Loss: 0.011605\n",
      "Epoch: 83, Loss: 0.002734\n",
      "Epoch: 84, Loss: 0.003691\n",
      "Epoch: 85, Loss: 0.006662\n",
      "Epoch: 86, Loss: 0.003586\n",
      "Epoch: 87, Loss: 0.001319\n",
      "Epoch: 88, Loss: 0.005720\n",
      "Epoch: 89, Loss: 0.000596\n",
      "Epoch: 90, Loss: 0.003573\n",
      "Epoch: 91, Loss: 0.001052\n",
      "Epoch: 92, Loss: 0.003124\n",
      "Epoch: 93, Loss: 0.003528\n",
      "Epoch: 94, Loss: 0.001890\n",
      "Epoch: 95, Loss: 0.001488\n",
      "Epoch: 96, Loss: 0.001861\n",
      "Epoch: 97, Loss: 0.013281\n",
      "Epoch: 98, Loss: 0.003708\n",
      "Epoch: 99, Loss: 0.000539\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3072, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 2)\n",
    ")\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # 使用这个方法可以提高acc\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64 64\n",
      "128 128\n",
      "192 192\n",
      "256 256\n",
      "320 320\n",
      "384 384\n",
      "448 448\n",
      "512 512\n",
      "576 576\n",
      "640 640\n",
      "704 704\n",
      "768 768\n",
      "832 832\n",
      "896 896\n",
      "960 960\n",
      "1024 1024\n",
      "1088 1088\n",
      "1151 1152\n",
      "1214 1216\n",
      "1278 1280\n",
      "1342 1344\n",
      "1406 1408\n",
      "1470 1472\n",
      "1534 1536\n",
      "1597 1600\n",
      "1661 1664\n",
      "1725 1728\n",
      "1789 1792\n",
      "1853 1856\n",
      "1917 1920\n",
      "1981 1984\n",
      "2045 2048\n",
      "2109 2112\n",
      "2173 2176\n",
      "2237 2240\n",
      "2301 2304\n",
      "2364 2368\n",
      "2427 2432\n",
      "2491 2496\n",
      "2555 2560\n",
      "2619 2624\n",
      "2683 2688\n",
      "2747 2752\n",
      "2811 2816\n",
      "2875 2880\n",
      "2939 2944\n",
      "3003 3008\n",
      "3066 3072\n",
      "3130 3136\n",
      "3194 3200\n",
      "3258 3264\n",
      "3322 3328\n",
      "3386 3392\n",
      "3450 3456\n",
      "3514 3520\n",
      "3578 3584\n",
      "3642 3648\n",
      "3706 3712\n",
      "3770 3776\n",
      "3834 3840\n",
      "3898 3904\n",
      "3962 3968\n",
      "4026 4032\n",
      "4090 4096\n",
      "4154 4160\n",
      "4218 4224\n",
      "4282 4288\n",
      "4346 4352\n",
      "4410 4416\n",
      "4474 4480\n",
      "4538 4544\n",
      "4602 4608\n",
      "4666 4672\n",
      "4730 4736\n",
      "4794 4800\n",
      "4858 4864\n",
      "4922 4928\n",
      "4986 4992\n",
      "5050 5056\n",
      "5114 5120\n",
      "5178 5184\n",
      "5242 5248\n",
      "5306 5312\n",
      "5370 5376\n",
      "5434 5440\n",
      "5498 5504\n",
      "5562 5568\n",
      "5626 5632\n",
      "5690 5696\n",
      "5754 5760\n",
      "5818 5824\n",
      "5882 5888\n",
      "5946 5952\n",
      "6010 6016\n",
      "6074 6080\n",
      "6138 6144\n",
      "6202 6208\n",
      "6266 6272\n",
      "6330 6336\n",
      "6394 6400\n",
      "6458 6464\n",
      "6522 6528\n",
      "6585 6592\n",
      "6649 6656\n",
      "6713 6720\n",
      "6776 6784\n",
      "6840 6848\n",
      "6904 6912\n",
      "6968 6976\n",
      "7032 7040\n",
      "7096 7104\n",
      "7160 7168\n",
      "7224 7232\n",
      "7288 7296\n",
      "7351 7360\n",
      "7415 7424\n",
      "7479 7488\n",
      "7542 7552\n",
      "7606 7616\n",
      "7670 7680\n",
      "7733 7744\n",
      "7797 7808\n",
      "7861 7872\n",
      "7925 7936\n",
      "7989 8000\n",
      "8053 8064\n",
      "8117 8128\n",
      "8181 8192\n",
      "8245 8256\n",
      "8308 8320\n",
      "8371 8384\n",
      "8435 8448\n",
      "8499 8512\n",
      "8563 8576\n",
      "8627 8640\n",
      "8691 8704\n",
      "8755 8768\n",
      "8818 8832\n",
      "8882 8896\n",
      "8946 8960\n",
      "9009 9024\n",
      "9073 9088\n",
      "9137 9152\n",
      "9201 9216\n",
      "9265 9280\n",
      "9329 9344\n",
      "9393 9408\n",
      "9457 9472\n",
      "9520 9536\n",
      "9584 9600\n",
      "9648 9664\n",
      "9712 9728\n",
      "9776 9792\n",
      "9840 9856\n",
      "9904 9920\n",
      "9968 9984\n",
      "9984 10000\n",
      "accuracy: 0.998400\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in train_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        print(correct, total)\n",
    "\n",
    "print('accuracy: %f' % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49 64\n102 128\n151 192\n204 256\n257 320\n304 384\n350 448\n405 512\n457 576\n509 640\n561 704\n617 768\n675 832\n724 896\n774 960\n826 1024\n879 1088\n925 1152\n982 1216\n1030 1280\n1087 1344\n1141 1408\n1193 1472\n1245 1536\n1296 1600\n1352 1664\n1406 1728\n1457 1792\n1513 1856\n1571 1920\n1625 1984\n1638 2000\naccuracy: 0.819000\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        outputs = model(imgs.view(imgs.shape[0], -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "        print(correct, total)\n",
    "\n",
    "print(\"accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3737474"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1574402"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "first_model = nn.Sequential(\n",
    "    nn.Linear(3072, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 2),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "\n",
    "sum([p.numel() for p in first_model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1573376"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "sum([p.numel() for p in nn.Linear(3072, 512).parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3146752"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "sum([p.numel() for p in nn.Linear(3072, 1024).parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1024, 3072]), torch.Size([1024]))"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "linear = nn.Linear(3072, 1024)\n",
    "\n",
    "linear.weight.shape, linear.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}